{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd   \n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 28, 28)\n",
      "[0 0 0 0 0 1 0 0 0 0]\n",
      "5\n",
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "train_x = x_train[:5000,:]\n",
    "print(train_x.shape)\n",
    "y = y_train[:5000]\n",
    "test_x = x_test[:1000,:]\n",
    "test_y = y_test[:1000]\n",
    "train_y = (np.arange(np.max(y) + 1) == y[:, None]).astype(int)\n",
    "print(train_y[0])\n",
    "print(y_train[0])\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convolutional forward pass\n",
    "#Relu and Sigmoid are activation functions, later I'll decide which one is performing better\n",
    "\n",
    "def sigmoid(x): \n",
    "    return(1/(1+np.exp(-x)))\n",
    "\n",
    "def relu(x):\n",
    "    x = (x > 0) * x\n",
    "    return(x)\n",
    "\n",
    "# Defining the function to perform the convolutional forwardpass\n",
    "# If you have any doubts in how CNN works i suggest you watch Andrew Ng youtube channel\n",
    "def conv_forward_pass(inputs, kernel, bias):\n",
    "    '''Calculate the convolutional foward pass using an activation function \n",
    "       Inputs:\n",
    "       layer_input -> A group of images\n",
    "       kernel -> a matrix containing the filters and number of filters (kernel_row, kernel_col, kernel_ch, n_filters)\n",
    "       bias -> Matrix with biases for the convolutional calculation\n",
    "       \n",
    "       Output:\n",
    "       Array containing the output of the images passing through the convolutional layer\n",
    "       Each output corresponds to a function(x)= activation(z + b)\n",
    "    '''\n",
    "    batches = len(inputs)\n",
    "    conv_outputs =[]\n",
    "    for batch in range(0,batches):\n",
    "        layer_input = inputs[batch] #passing a single image through the convolutional layer\n",
    "        if len(layer_input.shape) == 2: # in the case there input layer has only one channel, e.g black&white picture\n",
    "            z_row = layer_input.shape[0] - kernel.shape[0] + 1\n",
    "            z_col = layer_input.shape[1] - kernel.shape[1] + 1\n",
    "            if len(kernel.shape) == 2: # in the case there is only one filter in the convolutional layer\n",
    "                z = np.empty((z_row,z_col))\n",
    "                for i in range(0,z_row):\n",
    "                    for j in range(0,z_col):\n",
    "                        x= 0\n",
    "                        for ik in range(0,kernel.shape[0]):\n",
    "                            for ij in range(0,kernel.shape[1]):\n",
    "                                x += layer_input[i+ik,j+ij]*kernel[ik,ij]\n",
    "                        z[i,j]= x + bias\n",
    "            else:\n",
    "                nfilter = kernel.shape[-1]\n",
    "                z = np.empty((z_row,z_col,nfilter))\n",
    "                for f in range(0, nfilter):\n",
    "                    for i in range(0,z_row):\n",
    "                        for j in range(0,z_col):\n",
    "                            x= 0\n",
    "                            for ik in range(0,kernel.shape[0]):\n",
    "                                for ij in range(0,kernel.shape[1]):\n",
    "                                    x += layer_input[i+ik,j+ij]*kernel[ik,ij,f]\n",
    "                            z[i,j,f]= x + bias[f]\n",
    "        else: # in the case there input layer has more than one channel\n",
    "            layer_input_ch = layer_input.shape[2]     \n",
    "            z_row = layer_input.shape[0] - kernel.shape[0] + 1\n",
    "            z_col = layer_input.shape[1] - kernel.shape[1] + 1\n",
    "            if len(kernel.shape) == 3: # in the case there is only one filter in the convolutional layer\n",
    "                z = np.empty((z_row,z_col))\n",
    "                for i in range(0,z_row):\n",
    "                    for j in range(0,z_col):\n",
    "                        x= 0\n",
    "                        for ik in range(0,kernel.shape[0]):\n",
    "                            for ij in range(0,kernel.shape[1]):\n",
    "                                for ch in range(0,layer_input_ch):\n",
    "                                    x += layer_input[i+ik,j+ij,ch]*kernel[ik,ij,ch]\n",
    "                        z[i,j]= x + bias\n",
    "            else:\n",
    "                layer_input_ch = layer_input.shape[2]\n",
    "                nfilter = kernel.shape[-1]\n",
    "                z = np.empty((z_row,z_col,nfilter))\n",
    "                a = np.empty(z.shape)\n",
    "                for f in range(0, nfilter):\n",
    "                    for i in range(0,z_row):\n",
    "                        for j in range(0,z_col):\n",
    "                            for ik in range(0,kernel.shape[0]):\n",
    "                                for ij in range(0,kernel.shape[1]):\n",
    "                                    x= 0\n",
    "                                    for ch in range(0,layer_input_ch):\n",
    "                                        x += layer_input[i+ik,j+ij,ch]*kernel[ik,ij,ch,f]\n",
    "                            z[i,j,f]= x + bias[f]\n",
    "        a = sigmoid(z)\n",
    "        #a = relu(z)\n",
    "        conv_outputs.append(a)\n",
    "    conv_outputs = np.array(conv_outputs)\n",
    "    return(conv_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxpool_forward_pass(inputs, pool_filter_size, stride):\n",
    "    '''Calculate the maxpool foward pass\n",
    "       Inputs:\n",
    "       pool_input -> Input from previous layer\n",
    "       pool_filter_size -> integer of the row and col dimension of the pool filter\n",
    "       stride -> number of places the filter slides with in the pool_input dimensions\n",
    "       \n",
    "       Output:\n",
    "       Array containing the output of the previous layer passing through the maxpool layer\n",
    "    '''\n",
    "    batches = len(inputs)\n",
    "    output = []\n",
    "    for batch in range(0,batches):\n",
    "        pool_input = inputs[batch]\n",
    "        f_row = pool_filter_size\n",
    "        f_col = pool_filter_size\n",
    "        if len(pool_input.shape) == 2: # in the case there is no Channel dimension\n",
    "            pool_input_row = pool_input.shape[0]\n",
    "            pool_input_col = pool_input.shape[1]\n",
    "            pool_output_row = int(((pool_input_row - f_row)/stride)+1)\n",
    "            pool_output_col = int(((pool_input_col - f_col)/stride)+1)\n",
    "            pool_output = np.empty((pool_output_row, pool_output_col))\n",
    "            for i in range(0,pool_output_row):\n",
    "                for j in range(0,pool_output_col):\n",
    "                    x= []\n",
    "                    for ik in range(0,f_row):\n",
    "                        for ij in range(0,f_col):\n",
    "                            x.append(pool_input[(((i-1)*stride)+f_row)+ik,(((j-1)*stride)+f_col)+ij])\n",
    "                    pool_output[i,j] = max(x)\n",
    "        else:    \n",
    "            pool_input_row = pool_input.shape[0]\n",
    "            pool_input_col = pool_input.shape[1]\n",
    "            pool_input_ch = pool_input.shape[2]\n",
    "            pool_output_row = int(((pool_input_row - f_row)/stride)+1)\n",
    "            pool_output_col = int(((pool_input_col - f_col)/stride)+1)\n",
    "            pool_output_ch = pool_input_ch\n",
    "            pool_output = np.empty((pool_output_row, pool_output_col, pool_output_ch))\n",
    "            for f in range(0, pool_output_ch):\n",
    "                        for i in range(0,pool_output_row):\n",
    "                            for j in range(0,pool_output_col):\n",
    "                                x= []\n",
    "                                for ik in range(0,f_row):\n",
    "                                    for ij in range(0,f_col):\n",
    "                                        x.append(pool_input[(((i-1)*stride)+f_row)+ik,(((j-1)*stride)+f_col)+ij,f])\n",
    "                                pool_output[i,j,f] = max(x)\n",
    "        output.append(pool_output)                        \n",
    "    output = np.array(output)    \n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_foward_pass(input_flatten):\n",
    "    '''Transform input in a flatten array of one column'''\n",
    "    batches = len(input_flatten)\n",
    "    flatten_outputs=[]\n",
    "    for batch in range(0,batches):\n",
    "        flatten_outputs.append(np.reshape(input_flatten[batch],-1))    \n",
    "    flatten_outputs = np.array(flatten_outputs)\n",
    "    return(flatten_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_foward_pass(dense_input, n_output_nodes, weights, bias):\n",
    "    '''Calculate the outputs of a dense layer \n",
    "       Inputs:\n",
    "       dense_input -> result from previous layer\n",
    "       n_output_nodes -> number of output nodes\n",
    "       weights -> matrix of weights \n",
    "       bias - > Matrix with biases\n",
    "\n",
    "       Outputs:\n",
    "       Group of array of nodes considering a activation function of previous nodes * weights + biases \n",
    "    '''\n",
    "    batches = len(dense_input)\n",
    "    dense_outputs=[]\n",
    "    for batch in range(0,batches):\n",
    "        z = np.empty((n_output_nodes))\n",
    "        for i in range(0,n_output_nodes):\n",
    "            z[i] = (dense_input[batch] @ weights[:,i]) + bias[i]\n",
    "        #z = z/len(dense_input[batch])\n",
    "        a = sigmoid(z)\n",
    "        #a = relu(z)\n",
    "        dense_outputs.append(a)    \n",
    "    return(np.array(dense_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_softmax(x):\n",
    "    exps = np.exp(x - np.max(x))\n",
    "    return(exps / np.sum(exps))\n",
    "\n",
    "def output_foward_pass(dense_input, n_output_nodes, weights, bias):\n",
    "    '''Calculate the outputs of a dense layer\n",
    "       Inputs:\n",
    "       dense_input -> single result from previous layer\n",
    "       n_output_nodes -> number of output nodes\n",
    "       weights -> matrix of weights \n",
    "       bias - > Matrix with biases\n",
    "\n",
    "       Outputs:\n",
    "       Flatten array of nodes considering a softmax function of previous nodes * weights + biases \n",
    "    '''\n",
    "    batches = len(dense_input)\n",
    "    dense_outputs=[]\n",
    "    for batch in range(0,batches):\n",
    "        z = np.empty((n_output_nodes))\n",
    "        for i in range(0,z.shape[0]):\n",
    "            z[i] = dense_input[batch] @ weights[:,i] + bias[i]\n",
    "        a = stable_softmax(z)\n",
    "        dense_outputs.append(a)\n",
    "    dense_outputs = np.array(dense_outputs)\n",
    "    return(dense_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_labels(prediction,labels):\n",
    "    label_reshape = np.zeros(prediction.shape)\n",
    "    for i in range(0,len(prediction)):\n",
    "        label_reshape[i][labels[i]] = 1\n",
    "    return(label_reshape)\n",
    "\n",
    "def loss_SSE(prediction, labels):\n",
    "    loss_SSE = []\n",
    "    labels_reshape = reshape_labels(prediction, labels)\n",
    "    for i in range(0,len(prediction)):\n",
    "        loss_SSE.append(np.sum((prediction[i] - labels_reshape[i])**2))\n",
    "    return(np.mean(loss_SSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_loss(prediction, labels):\n",
    "    label_reshape = reshape_labels(prediction, labels)\n",
    "    grad_loss_out = []\n",
    "    for i in range(0,len(prediction)):\n",
    "        grad_loss_out.append(2*(prediction[i] - label_reshape[i]))\n",
    "    return(np.array(grad_loss_out))\n",
    "\n",
    "def grad_softmax(x):\n",
    "    '''Compute the gradient of the output layer over each z, Output = softmax(z)\n",
    "    Input -> Output layer you waht to do the gradient of softmax over\n",
    "    Outpu -> Matrix of the gradient of each outpu w.r.t each z\n",
    "    '''\n",
    "    grad_softmax =[]\n",
    "    for k in range(0,len(x)):\n",
    "        jacobian =np.empty((x.shape[-1],x.shape[-1]))\n",
    "        for i in range(0, jacobian.shape[0]):\n",
    "            for j in range(0, jacobian.shape[1]):\n",
    "                if i == j:\n",
    "                    jacobian[i][j] =  x[k][i] * (1 - x[k][j])\n",
    "                else:\n",
    "                    jacobian[i][j] =  x[k][i] * (0 - x[k][j])\n",
    "        grad_softmax.append(jacobian)            \n",
    "    return(np.array(grad_softmax))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_backpropagation(prediction, labels, previous_layer_weights, previous_layer_bias, previous_layer_outputs):\n",
    "    '''Return the gradients of the loss function w.r.t the previous weights, biases, and inputs.\n",
    "    Inputs:\n",
    "    Prediction -> Group prediction for the batch\n",
    "    Labels -> Correct answers for the predictions\n",
    "    Previous_layer_weights -> Matrix of weights from the previous layer\n",
    "    Previous_layer_bias -> Array of biases from the previous layer\n",
    "    Previous_layer_ouput -> Array of the output of previous layer, .e.g, this layer input\n",
    "    \n",
    "    Output:\n",
    "    Gradients for Weights, Biases and previous activation nodes\n",
    "    '''\n",
    "    \n",
    "    # Calculating the Weights Gradients\n",
    "    \n",
    "    #Step 1 - Calculate the Gradient of the loss w.r.t the outputs\n",
    "    grad_loss_outputs = grad_loss(prediction, labels)\n",
    "    \n",
    "    #Step 2 - Calculate the Gradient of each output w.r.t each z, where output[i] = softmax(z[i])\n",
    "    grad_outputs_z = grad_softmax(prediction)\n",
    "    \n",
    "    #Step 3 - Calculate the Gradient of each z w.r.t each weight, where z[i] = W[i,j]X[j] + W[i,j+1]X[j+1] ... + W[i,j+n]X[j+n] + b[i]  \n",
    "    grad_z_weights = previous_layer_outputs\n",
    "    \n",
    "    #Step 4 - Calculate the Gradient the loss w.r.t each weight using the chain rule\n",
    "    w_row = previous_layer_weights.shape[0]\n",
    "    w_col = previous_layer_weights.shape[1]\n",
    "    w_ch = len(prediction)\n",
    "    grad_w = np.empty((w_ch, w_row, w_col))\n",
    "    for ch in range(0,w_ch):\n",
    "        for i in range(0, w_row):\n",
    "            for j in range(0,w_col):\n",
    "                grad_w[ch,i,j] = (grad_loss_outputs[ch] @ grad_outputs_z[ch][:][j])* grad_z_weights[ch][i]\n",
    "    \n",
    "    #Step 5 - Calculate the average of the gradients\n",
    "    grad_loss_weight = np.empty((w_row,w_col))\n",
    "    for i in range(0,w_row):\n",
    "            for j in range(0,w_col):\n",
    "                grad_loss_weight[i,j] = np.mean(grad_w[:,i,j])\n",
    "    \n",
    "    #return(grad_loss_weight)\n",
    "    \n",
    "    # Calculating the Bias Gradients\n",
    "    \n",
    "    #Step 6 - Calculate the Gradient of each z w.r.t each bias, where z[i] = W[i,j]X[j] + W[i,j+1]X[j+1] ... + W[i,j+n]X[j+n] + b[i]  \n",
    "    grad_z_bias = 1\n",
    "    \n",
    "    #Step 7 - Calculate the Gradient the loss w.r.t each bias using the chain rule\n",
    "    b_col = len(previous_layer_bias)\n",
    "    b_ch = len(prediction)\n",
    "    grad_b = np.empty((b_ch, b_col))\n",
    "    for ch in range(0,b_ch):\n",
    "        for j in range(0,b_col):\n",
    "            grad_b[ch,j] = (grad_loss_outputs[ch] @ grad_outputs_z[ch][j])* grad_z_bias\n",
    "    \n",
    "    #Step 8 - Calculate the average of the gradients\n",
    "    grad_loss_bias = np.empty((b_col))\n",
    "    for j in range(0,b_col):\n",
    "        grad_loss_bias[j] = np.mean(grad_b[:,j])\n",
    "    \n",
    "    #return(grad_loss_bias)\n",
    "    \n",
    "    # Calculating the last layer Activation Node Gradient\n",
    "    \n",
    "    #Step 9 - Calculate the Gradient of each z w.r.t each A, where z[i] = W[i,j]A[j] + W[i,j+1]A[j+1] ... + W[i,j+n]A[j+n] + b[i] \n",
    "    grad_z_previous_activation = previous_layer_weights\n",
    "    \n",
    "    #Step 10 - Calculate the Gradient the loss w.r.t each previous activation layer, using the chain rule\n",
    "    a_row = previous_layer_outputs.shape[-1]\n",
    "    a_ch = len(prediction)\n",
    "    grad_a = np.zeros((a_ch, a_row))\n",
    "    for ch in range(0,a_ch):\n",
    "        for i in range(0,a_row):\n",
    "            for j in range(0, prediction.shape[-1]):\n",
    "                x = 0\n",
    "                for sj in range(0, grad_outputs_z.shape[-1]):\n",
    "                    x += (grad_loss_outputs[ch][j] * grad_outputs_z[ch][j][sj] * grad_z_previous_activation[i][j])\n",
    "                grad_a[ch,i] = x\n",
    "       \n",
    "    return(grad_loss_weight, grad_loss_bias, grad_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_flatten(x, row ,col ,ch):\n",
    "    '''Return array into matrix of batch,(i,j,ch) dimensions\n",
    "    Inputs:\n",
    "    x -> Array of (batch, y) dimension\n",
    "    (row, col ,ch) -> number of rows, columns and channels the output matrix must have\n",
    "    \n",
    "    Output -> Array of dimension (batch, (row,col,ch))\n",
    "    '''\n",
    "    inverse_flatten = []\n",
    "    for batch in range(0,x.shape[0]):\n",
    "        inverse_flatten.append(x[batch].reshape((row,col,ch)))\n",
    "    \n",
    "    return(np.array(inverse_flatten))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_maxpool(gradients, output_matrix, input_matrix, pool_filter_size, pool_stride):\n",
    "    inverse_maxpool = np.zeros(input_matrix.shape)\n",
    "    for batch in range(0, output_matrix.shape[0]):\n",
    "        for ch in range(0, output_matrix.shape[-1]):\n",
    "            for i in range(0, output_matrix.shape[1]):\n",
    "                for j in range(0, output_matrix.shape[2]):\n",
    "                    for i_slide in  range(0,pool_filter_size):\n",
    "                        for j_slide in  range(0,pool_filter_size):\n",
    "                            if output_matrix[batch][i][j][ch] == input_matrix[batch][((i-1)*pool_stride)+i_slide][((j-1)*pool_stride)+j_slide][ch]:\n",
    "                                inverse_maxpool[batch][((i-1)*pool_stride)+i_slide][((j-1)*pool_stride)+j_slide][ch] = gradients[batch][i][j][ch]\n",
    "    return(inverse_maxpool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_input_backpropagation(actual_layer_outputs, previous_layer_kernel_weights, previous_layer_bias, previous_layer_outputs, grad_loss_outputs):\n",
    "    '''Return the gradients of the loss function w.r.t the previous weights, biases, and inputs.\n",
    "    Inputs:\n",
    "    actual_layer_outputs -> Group of the outputs for this layer\n",
    "    Previous_layer__kernel_weights -> Matrix of weights from the convolutional kernel\n",
    "    Previous_layer_bias -> Array of biases from the previous layer\n",
    "    Previous_layer_ouput -> Array of the output of previous layer, .e.g, this layer input\n",
    "    grad_loss_outputs -> Array of the gradient of the loss function w.r.t. each node of this layer\n",
    "    \n",
    "    Output:\n",
    "    Gradients for Weights, Biases and previous activation nodes\n",
    "    '''\n",
    "    # Calculating the Weights Gradients\n",
    "    \n",
    "    #Step 1 - Calculate the Gradient of the loss w.r.t the layers node outputs\n",
    "    '''This step was previously calculated in the backpropagation of the output layer'''\n",
    "    #grad_loss_outputs = grad_loss_outputs\n",
    "    \n",
    "    #Step 2 - Calculate the Gradient of each node w.r.t each z, where output[i] = sigmoid(z[i])\n",
    "    # Deriviative of sigmoid function\n",
    "    grad_outputs_z = actual_layer_outputs * (1 - actual_layer_outputs)\n",
    "    #grad_outputs_z = grad_relu(actual_layer_outputs)\n",
    "    \n",
    "    #Step 3 - Calculate the Gradient the loss w.r.t each kernel weight using the chain rule  \n",
    "    batches = len(actual_layer_outputs)\n",
    "    n_kernels = previous_layer_kernel_weights.shape[-1]\n",
    "    k_row = previous_layer_kernel_weights.shape[0]\n",
    "    k_col = previous_layer_kernel_weights.shape[1]\n",
    "    \n",
    "    outputs_row = actual_layer_outputs.shape[1]\n",
    "    outputs_col = actual_layer_outputs.shape[2]\n",
    "       \n",
    "    grad_k = np.empty((batches, k_row, k_col, n_kernels))\n",
    "    for batch in range(0,batches):\n",
    "        for nk in range(0, n_kernels):\n",
    "            for i in range(0, k_row):\n",
    "                for j in range(0,k_col):\n",
    "                    grad_k[batch, i, j] = np.sum((grad_loss_outputs[batch][:,:,nk] * grad_outputs_z[batch][:,:,nk]) *  previous_layer_outputs[batch][i:i+outputs_row,j:j+outputs_col])\n",
    "    \n",
    "    #Step 4 - Calculate the average of the gradients\n",
    "    grad_loss_kernel = np.empty((k_row,k_col, n_kernels))\n",
    "    for nk in range(0, n_kernels):\n",
    "        for i in range(0, k_row):\n",
    "            for j in range(0,k_col):\n",
    "                grad_loss_kernel[i,j,nk] = np.mean(grad_k[:,i,j,nk])\n",
    "    \n",
    "    #return(grad_loss_kernel)\n",
    "    \n",
    "    #Step 5 - Calculate the Gradient of each z w.r.t each bias\n",
    "    grad_z_bias = 1\n",
    "    \n",
    "    #Step 6 - Calculate the Gradient the loss w.r.t each bias using the chain rule\n",
    "    grad_b = np.empty((batches, n_kernels))\n",
    "    for batch in range(0,batches):\n",
    "        for nk in range(0, n_kernels):\n",
    "            grad_b[batch,nk] = np.sum(grad_loss_outputs[batch][:,:,nk] * grad_outputs_z[batch][:,:,nk])\n",
    "    \n",
    "    #Step 7 - Calculate the average of the gradients\n",
    "    grad_loss_bias = np.empty((n_kernels))\n",
    "    for nk in range(0, n_kernels):\n",
    "        grad_loss_bias[nk] = np.mean(grad_b[:,nk])\n",
    "    \n",
    "    #return(grad_loss_bias)\n",
    "\n",
    "    return(grad_loss_kernel, grad_loss_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "\n",
    "def fit_CNN_small(training_features, training_labels, test_features, test_labels, epochs, batch_size, lr):\n",
    "    '''Fit the CNN architecture to the training data'''\n",
    "    #================================================================================================================\n",
    "    # Step 1: initialize weights, kernels, and biases\n",
    "    #================================================================================================================\n",
    "    #Layer 1\n",
    "    l1_kernel_size = 3\n",
    "    l1_kernel_numbers = 2\n",
    "    l1_kernel = np.random.randn(l1_kernel_size,\n",
    "                          l1_kernel_size,\n",
    "                          l1_kernel_numbers)\n",
    "    \n",
    "    l1_bias = np.random.randn(l1_kernel_numbers)\n",
    "    \n",
    "    #Layer 2\n",
    "    #N/A\n",
    "    \n",
    "    #Layer 3\n",
    "    #N/A\n",
    "    \n",
    "    #Layer 4 \n",
    "    output_nodes = 10\n",
    "    l4_weights =np.random.randn(338,output_nodes) #338 is the input shape from the flatten layer\n",
    "    l4_bias = np.random.randn(output_nodes)\n",
    "    \n",
    "    #================================================================================================================\n",
    "    # Step 2: divide the number of batches\n",
    "    #================================================================================================================\n",
    "    \n",
    "    number_of_batches = len(training_features)/batch_size\n",
    "    number_of_batches = math.ceil(number_of_batches)\n",
    "\n",
    "    #================================================================================================================\n",
    "    # Step 3: train for each epoch and batch\n",
    "    #================================================================================================================\n",
    "    metric=pd.DataFrame(columns=['Epoch', 'Train Loss', 'Train Accuracy', 'Test Loss', 'Test Accuracy'])\n",
    "    counter = 0\n",
    "    last_test_acc = 0\n",
    "    for epoch in range(0, epochs):\n",
    "        for i in range(0,number_of_batches-1):\n",
    "            # Forward Pass\n",
    "            l1_inputs = training_features[i*batch_size:i*batch_size + batch_size]\n",
    "            l2_inputs = conv_forward_pass(l1_inputs, l1_kernel, l1_bias)\n",
    "            l3_inputs = maxpool_forward_pass(l2_inputs, 2, 2)\n",
    "            l4_inputs = flatten_foward_pass(l3_inputs)\n",
    "            outputs = output_foward_pass(l4_inputs, output_nodes, l4_weights, l4_bias)\n",
    "\n",
    "            # Calculate Loss\n",
    "            batch_label = training_labels[i*batch_size:i*batch_size + batch_size]\n",
    "            batch_loss = loss_SSE(outputs, batch_label)\n",
    "            sys.stdout.write('\\rEpoch: %d ,Batch: %d ,loss: %.4f '%(epoch,i,batch_loss))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "\n",
    "            # Backpropagation\n",
    "            grad_l4_weights, grad_l4_bias, grad_l4_inputs = output_backpropagation(outputs, batch_label, l4_weights , l4_bias, l4_inputs)\n",
    "            grad_l3_inputs = inverse_flatten(grad_l4_inputs, 13, 13, 2)\n",
    "            grad_l2_inputs = inverse_maxpool(grad_l3_inputs, l3_inputs, l2_inputs, 2, 2)\n",
    "            grad_l1_kernel, grad_l1_bias = convolutional_input_backpropagation(l2_inputs, l1_kernel, l1_bias, l1_inputs, grad_l2_inputs)\n",
    "\n",
    "            #Updating Kernel, weights and biases\n",
    "            l1_kernel = l1_kernel - (lr*grad_l1_kernel)\n",
    "            l1_bias = l1_bias - (lr*grad_l1_bias)\n",
    "\n",
    "            l4_weights = l4_weights - (lr*grad_l4_weights)\n",
    "            l4_bias = l4_bias - (lr*grad_l4_bias)\n",
    "\n",
    "\n",
    "        # Calculate for last batch\n",
    "        # Forward Pass\n",
    "        l1_inputs = training_features[(number_of_batches-1)*batch_size:]\n",
    "        l2_inputs = conv_forward_pass(l1_inputs, l1_kernel, l1_bias)\n",
    "        l3_inputs = maxpool_forward_pass(l2_inputs, 2, 2)\n",
    "        l4_inputs = flatten_foward_pass(l3_inputs)\n",
    "        outputs = output_foward_pass(l4_inputs, output_nodes, l4_weights, l4_bias)\n",
    "\n",
    "        # Calculate Loss\n",
    "        batch_label = training_labels[(number_of_batches-1)*batch_size:]\n",
    "        batch_loss = loss_SSE(outputs, batch_label)\n",
    "        sys.stdout.write('\\rEpoch: %d ,Batch: %d ,loss: %.4f'%(epoch,number_of_batches,batch_loss))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        \n",
    "        # Backpropagation\n",
    "        grad_l4_weights, grad_l4_bias, grad_l4_inputs = output_backpropagation(outputs, batch_label, l4_weights , l4_bias, l4_inputs)\n",
    "        grad_l3_inputs = inverse_flatten(grad_l4_inputs, 13, 13, 2)\n",
    "        grad_l2_inputs = inverse_maxpool(grad_l3_inputs, l3_inputs, l2_inputs, 2, 2)\n",
    "        grad_l1_kernel, grad_l1_bias = convolutional_input_backpropagation(l2_inputs, l1_kernel, l1_bias, l1_inputs, grad_l2_inputs)\n",
    "        \n",
    "        #Updating Kernel, weights and biases\n",
    "        l1_kernel = l1_kernel - (lr*grad_l1_kernel)\n",
    "        l1_bias = l1_bias - (lr*grad_l1_bias)\n",
    "\n",
    "        l4_weights = l4_weights - (lr*grad_l4_weights)\n",
    "        l4_bias = l4_bias - (lr*grad_l4_bias)\n",
    "    \n",
    "    #================================================================================================================\n",
    "    # Step 4: Calculate Epoch train Loss\n",
    "    #================================================================================================================\n",
    "        # Calculate for Foward Pass for epoch\n",
    "        l1_inputs = training_features\n",
    "        l2_inputs = conv_forward_pass(l1_inputs, l1_kernel, l1_bias)\n",
    "        l3_inputs = maxpool_forward_pass(l2_inputs, 2, 2)\n",
    "        l4_inputs = flatten_foward_pass(l3_inputs)\n",
    "        outputs = output_foward_pass(l4_inputs, output_nodes, l4_weights, l4_bias)\n",
    "\n",
    "        # Calculate Epoch train Loss\n",
    "        batch_label = training_labels\n",
    "        batch_loss = loss_SSE(outputs, batch_label)\n",
    "    \n",
    "    #================================================================================================================\n",
    "    # Step 5: Calculate Epoch train Accuracy\n",
    "    #================================================================================================================\n",
    "    \n",
    "        outputs_acc = np.zeros(outputs.shape)\n",
    "        for i in range(0,len(outputs)):\n",
    "            outputs_acc[i][np.argmax(outputs[i])] = 1.\n",
    "\n",
    "        labels_reshape = reshape_labels(outputs, batch_label)\n",
    "        train_acc = np.sum(outputs_acc * labels_reshape)/len(outputs_acc)\n",
    "               \n",
    "     #================================================================================================================\n",
    "    # Step 6: Calculate Epoch test Loss\n",
    "    #================================================================================================================\n",
    "        # Calculate for Foward Pass for epoch\n",
    "        l1_inputs = test_features\n",
    "        l2_inputs = conv_forward_pass(l1_inputs, l1_kernel, l1_bias)\n",
    "        l3_inputs = maxpool_forward_pass(l2_inputs, 2, 2)\n",
    "        l4_inputs = flatten_foward_pass(l3_inputs)\n",
    "        outputs_test = output_foward_pass(l4_inputs, output_nodes, l4_weights, l4_bias)\n",
    "\n",
    "        # Calculate Epoch train Loss\n",
    "        batch_test_label = test_labels\n",
    "        batch_test_loss = loss_SSE(outputs_test, batch_label)   \n",
    "\n",
    "    #================================================================================================================\n",
    "    # Step 7: Calculate Epoch test Accuracy\n",
    "    #================================================================================================================\n",
    "        outputs_test_acc = np.zeros(outputs_test.shape)\n",
    "        for i in range(0,len(outputs_test)):\n",
    "            outputs_test_acc[i][np.argmax(outputs_test[i])] = 1.\n",
    "\n",
    "        labels_reshape = reshape_labels(outputs_test, batch_test_label)\n",
    "        test_acc = np.sum(outputs_test_acc * labels_reshape)/len(outputs_test_acc)\n",
    "        \n",
    "        print('\\nEpoch:',epoch,'\\tTrain_loss:',batch_loss, '\\tTrain_acc:',train_acc,'\\tTest_loss:',batch_test_loss, '\\tTest_acc:',test_acc)\n",
    "        metric.loc[counter] = [epoch, batch_loss, train_acc, batch_test_loss, test_acc]\n",
    "        counter +=1\n",
    "        \n",
    "        \n",
    "    return(l1_kernel, l1_bias, l4_weights, l4_bias, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mansi\\anaconda3\\envs\\project\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning: overflow encountered in exp\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 ,Batch: 79 ,loss: 1.4212 \n",
      "Epoch: 0 \tTrain_loss: 1.3961307582141047 \tTrain_acc: 0.7286 \tTest_loss: 1.4395291456613255 \tTest_acc: 0.1\n",
      "Epoch: 1 ,Batch: 35 ,loss: 1.1071 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-a8909b37125c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ml1_kernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml1_bias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml4_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml4_bias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_CNN_small\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1e-02\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-33-7edf77262184>\u001b[0m in \u001b[0;36mfit_CNN_small\u001b[1;34m(training_features, training_labels, test_features, test_labels, epochs, batch_size, lr)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[1;31m# Backpropagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m             \u001b[0mgrad_l4_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_l4_bias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_l4_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_backpropagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml4_weights\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0ml4_bias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml4_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m             \u001b[0mgrad_l3_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minverse_flatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_l4_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m13\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m13\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mgrad_l2_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minverse_maxpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_l3_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml3_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml2_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-2f84c0c2c0ff>\u001b[0m in \u001b[0;36moutput_backpropagation\u001b[1;34m(prediction, labels, previous_layer_weights, previous_layer_bias, previous_layer_outputs)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_row\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw_col\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m                 \u001b[0mgrad_w\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mgrad_loss_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mch\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mgrad_outputs_z\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m \u001b[0mgrad_z_weights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;31m#Step 5 - Calculate the average of the gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "l1_kernel, l1_bias, l4_weights, l4_bias, metric = fit_CNN_small(train_x, train_y, test_x, test_y, 2, 64, 1e-02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
